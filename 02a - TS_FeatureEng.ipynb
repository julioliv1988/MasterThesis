{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f86c4b3-53bf-4845-b3bd-c8c5c6c14aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71817acf-ba6c-4891-ab97-b018f6dbf3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "from logic import *\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe9b7b1-97bf-487a-b4f3-f620de7a2b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e188014-036a-430c-acd6-7cfdd6eaf394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"TS_FeatureEng\", \"False\",[\"True\",\"False\"],label=None)\n",
    "dbutils.widgets.dropdown(\"TS_store_split_raw\", \"False\",[\"True\",\"False\"],label=None)\n",
    "dbutils.widgets.dropdown(\"TS_store_feature_eng\", \"False\",[\"True\",\"False\"],label=None)\n",
    "dbutils.widgets.text(\"SerieNumber\",defaultValue=\"0\")\n",
    "dbutils.widgets.text(\"Modified_serie\",defaultValue=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0012c9d0-f461-4ebe-b6c7-0f05b9de1b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TS_FeatureEng = dbutils.widgets.get(\"TS_FeatureEng\") in ['True']\n",
    "TS_store_split_raw = dbutils.widgets.get(\"TS_store_split_raw\") in ['True']\n",
    "TS_store_feature_eng = dbutils.widgets.get(\"TS_store_feature_eng\") in ['True']\n",
    "SerieNumber = dbutils.widgets.get('SerieNumber')\n",
    "Modified_serie = ast.literal_eval(dbutils.widgets.get('Modified_serie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb898f15-44d8-4dff-b297-1e1620b72d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Notebook validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4532ec36-1b20-474c-85f9-5224db50763e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When running the job (workflow) this block checks if the rest of the notebook should be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71d5ade-9030-4072-a70e-a581130a5190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generating a random string to identify this run of the workflow\n",
    "from logic import generate_random_string\n",
    "_random_string_ = generate_random_string(7)\n",
    "print(_random_string_)\n",
    "dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/Experiments_info/{_random_string_}')\n",
    "dbutils.jobs.taskValues.set(key=\"job_reference\", value=_random_string_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd1bbe2-b241-4f53-b39a-9f65fbd9a601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not TS_FeatureEng:\n",
    "    dbutils.notebook.exit(\"Skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e27b8d-65f8-4430-b0ba-2705a7645af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Verifying setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c9f124-732c-48e4-af40-d152a9e41f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TS_store_split_raw -> tells if we are storing the split time-series (without engineered features) in the container\n",
    "\n",
    "TS_store_feature_eng -> tells if we are storing the splited time-serie (with engineered features) in the container\n",
    "\n",
    "Subsequent tasks (like regression run) use the stored data, then TS_store_feature_eng is just reccomended to be false just when troubleshooting mode on notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d91c4b-fef3-4b60-9057-a6ec63d6cd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"TS_store_feature_eng: {TS_store_feature_eng}\")\n",
    "print(f\"TS_store_split_raw: {TS_store_split_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a2febae-982c-4fa7-9ad7-2d27117c4415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HejPh6C6CrXv"
   },
   "source": [
    "#Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225397e8-04c8-4808-ab2f-8a0c74a72977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not Modified_serie:\n",
    "    decorator = \"\"\n",
    "else:\n",
    "    decorator = f\"-{Modified_serie[0]}-{Modified_serie[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c24cd7-87bb-4444-ad00-f2ad4dfa2b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(decorator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4cee189-2932-4d53-bd5c-dab5a459f962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "959a9f3f-284c-4886-970f-50c09de664e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class timeseries_solver:\n",
    "      def __init__(self, _df_, _SerieNumber_):\n",
    "\n",
    "        self.df = _df_\n",
    "        self.df_engineered = 0\n",
    "        self.list_train = 0\n",
    "        self.list_test = 0\n",
    "\n",
    "        self.y_train_list_expanding = 0\n",
    "        self.x_train_list_expanding = 0\n",
    "        self.y_test_list_expanding = 0\n",
    "        self.x_test_list_expanding = 0\n",
    "\n",
    "        self.regressor_list = []\n",
    "        self.y_pred_list = []\n",
    "\n",
    "        self.frequencies_calculated = []\n",
    "\n",
    "        self.SerieNumber = _SerieNumber_\n",
    "\n",
    "        #No season -> for comparison\n",
    "\n",
    "        self.regressor_list_no_season = []\n",
    "        self.y_pred_list_no_season = []\n",
    "\n",
    "        self.list_train_no_season = []\n",
    "        self.list_test_no_season = []\n",
    "\n",
    "        self.y_train_list_no_season_expanding = []\n",
    "        self.x_train_list_no_season_expanding = []\n",
    "        self.y_test_list_no_season_expanding = []\n",
    "        self.x_test_list_no_season_expanding = []\n",
    "\n",
    "\n",
    "        self.list_of_models = []\n",
    "\n",
    "        # Error Metrics\n",
    "\n",
    "        self.no_season_mae = 0\n",
    "        self.no_season_mse = 0\n",
    "        self.no_season_rmse = 0\n",
    "        self.no_season_r2 = 0\n",
    "        self.no_season_smape = 0\n",
    "\n",
    "        self.naive_mae = 0\n",
    "        self.naive_mse = 0\n",
    "        self.naive_rmse = 0\n",
    "        self.naive_r2 = 0\n",
    "        self.naive_smape = 0\n",
    "\n",
    "        self.regression_mae = 0\n",
    "        self.regression_mse = 0\n",
    "        self.regression_rmse = 0\n",
    "        self.regression_r2 = 0\n",
    "        self.regression_smape = 0\n",
    "\n",
    "        self.s_path = f'/mnt/automated_mounts_sas/0juliostoragetest/master_data'\n",
    "        \n",
    "      def expanding_split(self,test_size,train_size):\n",
    "\n",
    "        # test_size  -> forecast horizont, how many steps(points) in the future we'll forecast\n",
    "        # train_size -> How many points we'll use to train our models in each window\n",
    "\n",
    "        df_length = self.df.shape[0]\n",
    "\n",
    "        n_windows = math.floor((df_length-train_size)/test_size)\n",
    "\n",
    "        list_i_train = []\n",
    "        list_f_train = []\n",
    "        list_i_test = []\n",
    "        list_f_test = []\n",
    "\n",
    "        for i in range(n_windows):\n",
    "          list_i_train.append(i*test_size)\n",
    "\n",
    "        for i in list_i_train:\n",
    "          list_f_train.append(i+(train_size-1))\n",
    "\n",
    "        list_i_train = []\n",
    "        for i in range(n_windows):\n",
    "          list_i_train.append(0)\n",
    "\n",
    "        for i in list_f_train:\n",
    "          list_i_test.append(i+1)\n",
    "\n",
    "        for i in list_i_test:\n",
    "          list_f_test.append(i+(test_size-1))\n",
    "\n",
    "        List_lista_treino = []\n",
    "        List_lista_teste = []\n",
    "\n",
    "        for i in range(n_windows):\n",
    "\n",
    "            list_treino = np.arange(list_i_train[i], list_f_train[i]+1)\n",
    "            list_teste = np.arange(list_i_test[i], list_f_test[i]+1)\n",
    "\n",
    "            a_train_index = list_treino\n",
    "            test_index = list_teste\n",
    "            a_train_index = a_train_index.tolist()\n",
    "            test_index = test_index.tolist()\n",
    "            List_lista_treino.append(list_treino)\n",
    "            List_lista_teste.append(list_teste)\n",
    "\n",
    "        self.df = self.df.copy()\n",
    "        self.df['index'] = self.df.copy().index\n",
    "\n",
    "        lista_dfs_treino = []\n",
    "        lista_dfs_teste = []\n",
    "\n",
    "        if TS_store_split_raw:\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set/',True)\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set/',True)\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set')\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set')\n",
    "\n",
    "        for i in range(len(List_lista_treino)):\n",
    "          lista_dfs_treino.append(self.df[self.df['index'].isin(List_lista_treino[i])])\n",
    "          if TS_store_split_raw:          \n",
    "            lista_dfs_treino[-1].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set/window_{i}.csv\")\n",
    "\n",
    "          lista_dfs_teste.append(self.df[self.df['index'].isin(List_lista_teste[i])])\n",
    "          if TS_store_split_raw:\n",
    "            lista_dfs_teste[-1].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set/window_{i}.csv\")\n",
    "\n",
    "        self.list_train = lista_dfs_treino\n",
    "        self.list_test = lista_dfs_teste\n",
    "\n",
    "        # self.y_train_list_expanding, self.x_train_list_expanding, self.y_test_list_expanding, self.x_test_list_expanding = train_test_convert_2_regressor(train_train,test_list)\n",
    "\n",
    "      def feature_eng(self):\n",
    "        for i in range(len(self.list_train)):\n",
    "\n",
    "          train_partial = self.list_train[i].copy()\n",
    "\n",
    "          ########list_train\n",
    "          ##################non-seasonal features\n",
    "          train_partial['Last-observation'] = train_partial['observation'].shift(+1)\n",
    "\n",
    "          train_partial['SMA-1'] = train_partial.observation.rolling(window=20).mean()\n",
    "          train_partial['SMA-1'] = train_partial['SMA-1'].shift(+1)\n",
    "\n",
    "          train_partial['EMA-1'] = train_partial.observation.ewm(alpha=0.1, adjust=False).mean()\n",
    "          train_partial['EMA-2'] = train_partial.observation.ewm(alpha=0.45, adjust=False).mean()\n",
    "          train_partial['EMA-3'] = train_partial.observation.ewm(alpha=0.9, adjust=False).mean()\n",
    "\n",
    "          train_partial['EMA-1'] = train_partial['EMA-1'].shift(+1)\n",
    "          train_partial['EMA-2'] = train_partial['EMA-2'].shift(+1)\n",
    "          train_partial['EMA-3'] = train_partial['EMA-3'].shift(+1)\n",
    "\n",
    "          train_partial['std-1'] = train_partial.observation.rolling(window=20).std()\n",
    "          train_partial['std-1'] = train_partial['std-1'].shift(+1)\n",
    "\n",
    "          # Min - Max\n",
    "          train_partial[\"last_max\"] = train_partial.observation.rolling(window=20).max()\n",
    "          train_partial['last_max'] = train_partial['last_max'].shift(+1)\n",
    "\n",
    "          train_partial[\"last_min\"] = train_partial.observation.rolling(window=20).min()\n",
    "          train_partial['last_min'] = train_partial['last_min'].shift(+1)\n",
    "          # When we truncate the dataset below we may be removing the max min from the train window\n",
    "\n",
    "          train_partial = train_partial.iloc[20:]\n",
    "          train_partial = train_partial.copy() #--> added not to face warnings\n",
    "\n",
    "          train_partial[\"observation_detrended\"] = train_partial['Last-observation'] - train_partial['SMA-1']\n",
    "\n",
    "          train_partial[\"observation_detrended_non_neg\"] = train_partial[\"observation_detrended\"] - min(train_partial.observation_detrended)+10\n",
    "\n",
    "          # Compute the FFT\n",
    "          fft_result = np.fft.fft(train_partial[\"observation_detrended_non_neg\"])\n",
    "\n",
    "          # Frequency values corresponding to the FFT result\n",
    "\n",
    "          freq = np.fft.fftfreq(len(train_partial[\"observation_detrended_non_neg\"]), 1.0 / len(train_partial[\"observation_detrended_non_neg\"]))*1000/len(train_partial[\"observation_detrended_non_neg\"])\n",
    "\n",
    "          # Find the index of the maximum amplitude in the FFT result\n",
    "          index_of_max_amplitude = np.argmax(np.abs(fft_result))\n",
    "\n",
    "          # Extract the fundamental frequency\n",
    "          fundamental_frequency = np.abs(freq[index_of_max_amplitude])\n",
    "\n",
    "          peaks, _ = find_peaks(np.abs(fft_result), height=100, distance=10)\n",
    "          peak_frequencies = freq[peaks]\n",
    "          peak_frequencies = [x for x in peak_frequencies if x > 0]\n",
    "\n",
    "          self.frequencies_calculated.append(peak_frequencies)\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) == 0:\n",
    "            pass\n",
    "          else:\n",
    "            add_repeating_sequence_column(round(1000/self.frequencies_calculated[i][0]),train_partial)\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) >= 2:\n",
    "            add_repeating_sequence_column(round(1000/self.frequencies_calculated[i][1]),train_partial,column_name=\"Repeating_Sequence_2\")\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "          ammount_of_dummy_cols_Repeating_Sequence = 5\n",
    "          ammount_of_dummy_cols_Repeating_Sequence_2 = 5\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) == 0:\n",
    "            pass\n",
    "          else:\n",
    "            # For when the ammont of dummies matches the ammount of repeating sequences\n",
    "            # ammount_of_dummy_cols_Repeating_Sequence   = round(1000/self.frequencies_calculated[i][0])\n",
    "\n",
    "            train_partial = add_columns_dummy(train_partial.copy(), ammount_of_dummy_cols_Repeating_Sequence, \"Repeating_Sequence\")\n",
    "          if len(self.frequencies_calculated[i]) >= 2:\n",
    "            # For when the ammont of dummies matches the ammount of repeating sequences            \n",
    "            # ammount_of_dummy_cols_Repeating_Sequence_2 = round(1000/self.frequencies_calculated[i][1])\n",
    "\n",
    "            train_partial = add_columns_dummy(train_partial.copy(), ammount_of_dummy_cols_Repeating_Sequence_2, \"Repeating_Sequence_2\")\n",
    "          else: pass\n",
    "\n",
    "\n",
    "          # Group by the 'sequence' column and apply rolling average\n",
    "          #try 1st Repeating_Sequence_2 to keep with the shorter lenght period\n",
    "          try:\n",
    "            train_partial['seasonal_avg_2'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_avg(x, 2))\n",
    "            train_partial['seasonal_avg_2'] = train_partial['seasonal_avg_2'].shift(+1)\n",
    "            train_partial['seasonal_avg_3'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_avg(x, 3))\n",
    "            train_partial['seasonal_avg_3'] = train_partial['seasonal_avg_3'].shift(+1)\n",
    "\n",
    "            train_partial['seasonal_avg_diff'] = train_partial['seasonal_avg_2'] - train_partial['seasonal_avg_3']\n",
    "\n",
    "            train_partial['seasonal_std_2'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_std(x, 2))\n",
    "            train_partial['seasonal_std_2'] = train_partial['seasonal_std_2'].shift(+1)\n",
    "            train_partial['seasonal_std_3'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_std(x, 3))\n",
    "            train_partial['seasonal_std_3'] = train_partial['seasonal_std_3'].shift(+1)\n",
    "\n",
    "            train_partial['seasonal_std_diff'] = train_partial['seasonal_std_2'] - train_partial['seasonal_std_3']\n",
    "\n",
    "            train_partial['seasonal_min'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_min(x, 3))\n",
    "            train_partial['seasonal_min'] = train_partial['seasonal_min'].shift(+1)\n",
    "            train_partial['seasonal_max'] = train_partial.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_max(x, 3))\n",
    "            train_partial['seasonal_max'] = train_partial['seasonal_max'].shift(+1)\n",
    "\n",
    "          except:\n",
    "            try:\n",
    "              train_partial['seasonal_avg_2'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_avg(x, 2))\n",
    "              train_partial['seasonal_avg_2'] = train_partial['seasonal_avg_2'].shift(+1)\n",
    "              train_partial['seasonal_avg_3'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_avg(x, 3))\n",
    "              train_partial['seasonal_avg_3'] = train_partial['seasonal_avg_3'].shift(+1)\n",
    "\n",
    "              train_partial['seasonal_avg_diff'] = train_partial['seasonal_avg_2'] - train_partial['seasonal_avg_3']\n",
    "              \n",
    "              train_partial['seasonal_std_2'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_std(x, 2))\n",
    "              train_partial['seasonal_std_2'] = train_partial['seasonal_std_2'].shift(+1)\n",
    "              train_partial['seasonal_std_3'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_std(x, 3))\n",
    "              train_partial['seasonal_std_3'] = train_partial['seasonal_std_3'].shift(+1)\n",
    "\n",
    "              train_partial['seasonal_std_diff'] = train_partial['seasonal_std_2'] - train_partial['seasonal_std_3']\n",
    "\n",
    "              train_partial['seasonal_min'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_min(x, 3))\n",
    "              train_partial['seasonal_min'] = train_partial['seasonal_min'].shift(+1)\n",
    "              train_partial['seasonal_max'] = train_partial.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_max(x, 3))\n",
    "              train_partial['seasonal_max'] = train_partial['seasonal_max'].shift(+1)\n",
    "            except: pass\n",
    "\n",
    "\n",
    "          # remooving the rows containing nulls (from the rolling seasonal calculations)\n",
    "          train_partial = train_partial.dropna()\n",
    "\n",
    "          train_partial = reorder_df(train_partial)\n",
    "\n",
    "          # Falta replicar para o test e nao esquecer de organizar a ordem das colunas antes de escrever para storage\n",
    "          # falta resolver o problema dos nulls \n",
    "          # falta revisar!!!!\n",
    "\n",
    "          ########list_test\n",
    "\n",
    "          result = pd.concat([self.list_train[i], self.list_test[i]], ignore_index=True, sort=False)\n",
    "\n",
    "          result['Last-observation'] = result['observation'].shift(+1)\n",
    "\n",
    "          result['SMA-1'] = result.observation.rolling(window=20).mean()\n",
    "          result['SMA-1'] = result['SMA-1'].shift(+1)\n",
    "\n",
    "          result['EMA-1'] = result.observation.ewm(alpha=0.1, adjust=False).mean()\n",
    "          result['EMA-2'] = result.observation.ewm(alpha=0.45, adjust=False).mean()\n",
    "          result['EMA-3'] = result.observation.ewm(alpha=0.9, adjust=False).mean()\n",
    "\n",
    "          result['EMA-1'] = result['EMA-1'].shift(+1)\n",
    "          result['EMA-2'] = result['EMA-2'].shift(+1)\n",
    "          result['EMA-3'] = result['EMA-3'].shift(+1)\n",
    "\n",
    "          result['std-1'] = result.observation.rolling(window=20).std()\n",
    "          result['std-1'] = result['std-1'].shift(+1)\n",
    "\n",
    "          # Min - Max\n",
    "          result[\"last_max\"] = result.observation.rolling(window=20).max()\n",
    "          result['last_max'] = result['last_max'].shift(+1)\n",
    "\n",
    "          result[\"last_min\"] = result.observation.rolling(window=20).min()\n",
    "          result['last_min'] = result['last_min'].shift(+1)\n",
    "\n",
    "          result = result.iloc[20:]\n",
    "          result = result.copy() #--> added not to face warnings\n",
    "\n",
    "\n",
    "          # result = result.iloc[20:]\n",
    "          result[\"observation_detrended\"] = result['Last-observation'] - result['SMA-1']\n",
    "\n",
    "          result[\"observation_detrended_non_neg\"] = result[\"observation_detrended\"] - min(result.observation_detrended)+10\n",
    "\n",
    "\n",
    "          # Identify columns with 'SI' in their names\n",
    "          cols_to_drop = result.filter(like='SI').columns\n",
    "          # Drop the identified columns\n",
    "          result = result.drop(columns=cols_to_drop)\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) == 0:\n",
    "            pass\n",
    "          else:\n",
    "            add_repeating_sequence_column(round(1000/self.frequencies_calculated[i][0]), result)\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) >= 2:\n",
    "            add_repeating_sequence_column(round(1000/self.frequencies_calculated[i][1]),result, column_name=\"Repeating_Sequence_2\")\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "          if len(self.frequencies_calculated[i]) == 0:\n",
    "            pass\n",
    "          else:\n",
    "            # For when the ammont of dummies matches the ammount of repeating sequences\n",
    "            # ammount_of_dummy_cols_Repeating_Sequence   = round(1000/self.frequencies_calculated[i][0])\n",
    "\n",
    "            result = add_columns_dummy(result.copy(), ammount_of_dummy_cols_Repeating_Sequence, \"Repeating_Sequence\")\n",
    "          if len(self.frequencies_calculated[i]) >= 2:\n",
    "            # For when the ammont of dummies matches the ammount of repeating sequences\n",
    "            # ammount_of_dummy_cols_Repeating_Sequence_2 = round(1000/self.frequencies_calculated[i][1])\n",
    "\n",
    "            result = add_columns_dummy(result.copy(), ammount_of_dummy_cols_Repeating_Sequence, \"Repeating_Sequence_2\")\n",
    "          else: pass\n",
    "\n",
    "          # Group by the 'sequence' column and apply rolling average\n",
    "          #try 1st Repeating_Sequence_2 to keep with the shorter lenght period\n",
    "          try:\n",
    "            result['seasonal_avg_2'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_avg(x, 2))\n",
    "            result['seasonal_avg_2'] = result['seasonal_avg_2'].shift(+1)\n",
    "            result['seasonal_avg_3'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_avg(x, 3))\n",
    "            result['seasonal_avg_3'] = result['seasonal_avg_3'].shift(+1)\n",
    "\n",
    "            result['seasonal_avg_diff'] = result['seasonal_avg_2'] - result['seasonal_avg_3']\n",
    "\n",
    "            result['seasonal_std_2'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_std(x, 2))\n",
    "            result['seasonal_std_2'] = result['seasonal_std_2'].shift(+1)\n",
    "            result['seasonal_std_3'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_std(x, 3))\n",
    "            result['seasonal_std_3'] = result['seasonal_std_3'].shift(+1)\n",
    "\n",
    "            result['seasonal_std_diff'] = result['seasonal_std_2'] - result['seasonal_std_3']\n",
    "\n",
    "            result['seasonal_min'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_min(x, 3))\n",
    "            result['seasonal_min'] = result['seasonal_min'].shift(+1)\n",
    "            result['seasonal_max'] = result.groupby('Repeating_Sequence_2')['observation'].apply(lambda x: compute_rolling_max(x, 3))\n",
    "            result['seasonal_max'] = result['seasonal_max'].shift(+1)\n",
    "\n",
    "          except:\n",
    "            try:\n",
    "              result['seasonal_avg_2'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_avg(x, 2))\n",
    "              result['seasonal_avg_2'] = result['seasonal_avg_2'].shift(+1)\n",
    "              result['seasonal_avg_3'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_avg(x, 3))\n",
    "              result['seasonal_avg_3'] = result['seasonal_avg_3'].shift(+1)\n",
    "\n",
    "              result['seasonal_avg_diff'] = result['seasonal_avg_2'] - result['seasonal_avg_3']\n",
    "\n",
    "              result['seasonal_std_2'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_std(x, 2))\n",
    "              result['seasonal_std_2'] = result['seasonal_std_2'].shift(+1)\n",
    "              result['seasonal_std_3'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_std(x, 3))\n",
    "              result['seasonal_std_3'] = result['seasonal_std_3'].shift(+1)\n",
    "\n",
    "              result['seasonal_std_diff'] = result['seasonal_std_2'] - result['seasonal_std_3']\n",
    "\n",
    "              result['seasonal_min'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_min(x, 3))\n",
    "              result['seasonal_min'] = result['seasonal_min'].shift(+1)\n",
    "              result['seasonal_max'] = result.groupby('Repeating_Sequence')['observation'].apply(lambda x: compute_rolling_max(x, 3))\n",
    "              result['seasonal_max'] = result['seasonal_max'].shift(+1)\n",
    "            except: pass\n",
    "\n",
    "          self.list_test[i] = result.tail(1)\n",
    "          # Identify columns with 'Repeating_Sequence' in their names\n",
    "          repeating_sequence_columns = [col for col in self.list_test[i].columns if 'Repeating_Sequence' in col]\n",
    "          non_repeating_sequence_columns = [col for col in self.list_test[i].columns if 'Repeating_Sequence' not in col]\n",
    "\n",
    "          # Reorder columns\n",
    "          new_column_order = non_repeating_sequence_columns + repeating_sequence_columns\n",
    "          self.list_test[i] = self.list_test[i][new_column_order]\n",
    "\n",
    "          # remooving unecessary columns\n",
    "          train_partial = train_partial.drop('observation_detrended', axis=1)\n",
    "          train_partial = train_partial.drop('observation_detrended_non_neg', axis=1)\n",
    "\n",
    "          self.list_train[i] = train_partial\n",
    "\n",
    "          self.list_test[i] = self.list_test[i].drop('observation_detrended', axis=1)\n",
    "          self.list_test[i] = self.list_test[i].drop('observation_detrended_non_neg', axis=1)\n",
    "        test_list = self.list_test.copy()\n",
    "        train_list = self.list_train.copy()\n",
    "\n",
    "        if TS_store_feature_eng:\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured/',True)\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured/',True)\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured')\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured')\n",
    "\n",
    "          for i in range(len(test_list)):\n",
    "            test_list[i].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured/window_{i}.csv\")\n",
    "            train_list[i].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured/window_{i}.csv\")\n",
    "\n",
    "      def feature_eng_no_season(self):\n",
    "\n",
    "        if TS_store_feature_eng:\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured_no_season/',True)\n",
    "          dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured_no_season/',True)\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured_no_season')\n",
    "          dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured_no_season')\n",
    "\n",
    "        for i in range(len(self.list_train)):\n",
    "\n",
    "          self.list_train_no_season.append(self.list_train[i][[\"observation\",\t\"index\",\t\"Last-observation\",\t\"SMA-1\",\t\"EMA-1\",\t\"EMA-2\",\t\"EMA-3\",\t\"std-1\",\t\"last_max\",\t\"last_min\"]])\n",
    "\n",
    "          self.list_test_no_season.append(self.list_test[i][[\"observation\",\t\"index\",\t\"Last-observation\",\t\"SMA-1\",\t\"EMA-1\",\t\"EMA-2\",\t\"EMA-3\",\t\"std-1\",\t\"last_max\",\t\"last_min\"]])\n",
    "\n",
    "          if TS_store_feature_eng:          \n",
    "            self.list_train_no_season[i].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/train_set_featured_no_season/window_{i}.csv\")\n",
    "            self.list_test_no_season[i].to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{self.SerieNumber}{decorator}/test_set_featured_no_season/window_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781885fa-536f-49b9-b45e-4f401f84b8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not Modified_serie:\n",
    "    traintest_df = pd.read_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/Input/input_{SerieNumber}.csv\", index_col=0)\n",
    "else:\n",
    "    traintest_df = pd.read_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/Input/input_{SerieNumber}.csv\", index_col=0)\n",
    "    traintest_df[\"observation\"] = traintest_df[Modified_serie[0]] + traintest_df[Modified_serie[1]]\n",
    "\n",
    "# Feature engineer just the 1st Serie\n",
    "traintest_df = traintest_df[[\"observation\"]]\n",
    "# traintest_df\n",
    "serie_solv = timeseries_solver(traintest_df, SerieNumber)\n",
    "serie_solv.expanding_split(1,40)\n",
    "serie_solv.feature_eng()\n",
    "serie_solv.feature_eng_no_season()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686dbbed-82a5-4d3f-b7a1-6cb9d11c9310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CDlq9Chj490p"
   },
   "outputs": [],
   "source": [
    "# Find the length of the longest list\n",
    "max_length = max(len(lst) for lst in serie_solv.frequencies_calculated)\n",
    "\n",
    "columns_freq = []\n",
    "for i in range(max_length):\n",
    "    columns_freq.append(\"f\" + str(i+1))\n",
    "freq_calculated = pd.DataFrame(serie_solv.frequencies_calculated, columns=columns_freq)\n",
    "# freq_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd22014-3e50-4305-b8ff-1da443fff867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Storing the CSVs\n",
    "dbutils.fs.rm(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{SerieNumber}{decorator}/Feature_eng/',True)\n",
    "dbutils.fs.mkdirs(f'/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{SerieNumber}{decorator}/Feature_eng')\n",
    "freq_calculated.to_csv(f\"/dbfs/mnt/automated_mounts_sas/0juliostoragetest/julio/master_data/SerieNumber_{SerieNumber}{decorator}/Feature_eng/freq_calculated.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02a - TS_FeatureEng",
   "widgets": {
    "Modified_serie": {
     "currentValue": " [\"trend\", \"season\"]",
     "nuid": "4a3434ae-edc6-49a2-8410-21c6eed1955e",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "False",
      "label": null,
      "name": "Modified_serie",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SerieNumber": {
     "currentValue": "1",
     "nuid": "4afde278-8405-4648-acbd-d26b5be58a47",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": null,
      "name": "SerieNumber",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TS_FeatureEng": {
     "currentValue": "True",
     "nuid": "a0f34ee7-b057-4e39-add7-0635b4c0cc5c",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": null,
      "name": "TS_FeatureEng",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "TS_store_feature_eng": {
     "currentValue": "True",
     "nuid": "80992c6f-f2df-4b73-96b6-9048db6e82cf",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": null,
      "name": "TS_store_feature_eng",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "TS_store_split_raw": {
     "currentValue": "True",
     "nuid": "55c944ec-768a-4632-a797-8653676c4c23",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": null,
      "name": "TS_store_split_raw",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
